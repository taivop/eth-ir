\documentclass[a4paper, 11pt]{article}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}
\newcommand{\lec}[2]{{\scriptsize \fcolorbox{black}{gray!30}{\textbf{L#1}.#2}}}

\title{Information Retrieval AS 2015: summary}
\author{Taivo Pungas}
\date{\today}

\begin{document}
\maketitle




\section{Near-duplicate detection}

We want to detect which documents in a database are almost exact duplicates of each other.

Challenges: \lec{2}{5-6}
\begin{enumerate}
	\item Use a robust similarity measure w.r.t. small variations: whitespace, punctuation, misspeelling, capitalisation, small text insertions/deletions.
	\item Avoid quadratic blow-up (all pairwise comparisons).
\end{enumerate}

Solution: summarise documents into sketches that can be efficiently clustered into near-duplicates. \lec{2}{6}

Possible sketch: character n-grams \lec{2}{6}
\begin{itemize}
	\item Pros: no tokenisation needed, language agnostic
	\item Cons: not space-efficient, hard to tune trade-offs, no inherent semantics (compared to words)
\end{itemize}

Shingles (word q-grams) work better. Each document is represented by the set of q-length token sequences that occur in it.

Jaccard coefficient as similarity measure: \lec{2}{7}
$$\textrm{sim}(A,B) := J(A, B) = \frac{|A \cap B|}{|A \cup B|} \in [0,1]$$

\subsection{Similarity hashing}
If we choose the right hash function $h$, then $J(S_A, S_B) = Pr[h(S_A) = h(S_B)]$. If we sample enough different $h$-s, we get an accurate estimate of $J(S_A, S_B)$. Special case of locality-sensitive hashing.

MinHash: early approach, hard to apply in practice, heuristic approach used. \lec{2}{7}
SimHash: better approach, combines hashed shingles into a single bitstring by majority voting. See \lec{2}{8}.

Use sorted tables of permuted fingerprints to do quick near-duplicate detection given a query doc.




\section{Scoring search results}

\subsection{Preprocessing documents and queries}
We can preprocess documents and queries by:
\begin{itemize}
		\item Stemming aka lemmatisation: reducing words to their stems. E.g. Porter stemmer. \lec{2}{4}
		\item Tokenising: e.g. handling numbers/dates/names as special cases. \lec{2}{4}
		\item Removing stop words (common terms like "the", "a", etc). \lec{2}{4}
\end{itemize}

How to order the documents our search engine retrieved such that the most relevant ones come first? Use a score function that assigns a relevancy score to each document, and produces a result list by returning documents with the highest scores.

\subsection{Evaluating scoring functions: precision and recall}
How to get ground truth data about relevance? We could get opinions from humans, but there are many challenges \lec{3}{1}. Still widely used though. We can also infer relevance from user click behavior.

When we have ground truth data, we can use precision and recall metrics: \lec{3}{1}

\begin{align}
\textrm{Precision} &= \frac{\textrm{TP}}{\textrm{TP + FP}} = \frac{\textrm{\# relevant items retrieved}}{\textrm{\# items retrieved}}\\
\textrm{Recall} &= \frac{\textrm{TP}}{\textrm{TP + FN}} = \frac{\textrm{\# relevant items retrieved}}{\textrm{\# relevant items in collection}}
\end{align}

Precision/recall curve: for each $i$, take top $i$ results, calculate precision and recall and plot result. P/R curves also generalise to ranked lists, not just sets, which makes them better for assessing search results. \lec{3}{2-3}

In web search, we want high precision since user looks at mainly top 2-3 results. Lower recall is more tolerable. There are also use cases where we might want to have very high or perfect recall. \lec{3}{2}

\paragraph{Evaluating other things than precision/recall} We can A/B test two systems: users tell us which result page is better. This allows us to compare whole result \emph{pages}, identify failures and also combine it with relevance assessments. Possible to do importance sampling (ask users to provide assessments that are especially important to us, e.g. ones we're most unsure about). \lec{3}{3}

\subsection{Naive scoring}
$\textrm{Score = \# common terms between query and document}$, with tie-breaking.  \lec{2}{8-9}

\subsection{tf-idf scoring}
The tf-idf score takes into account two things: \lec{4}{1-3}
\begin{itemize}
		\item How much the query term occurs in the given document (term frequency).
		\item How common the query term is in the entire collection (document frequency).
\end{itemize}

Empirically, raw term frequencies give bad weights. For this reason, we apply a shift and logarithm: \lec{4}{1}
$$\textrm{log-tf}(w;d) = \log(1 + \textrm{tf}(w; d))$$

Query terms that are found in almost all documents are less informative, so we want to decrease the weights of more common query terms. Let's take the inverse of document frequencies and logarithm it (common way to do it) \lec{4}{2}. Assume we have $n$ documents.

$$\textrm{df}(w) = \textrm{\# \{documents that contain } w\} \leq n$$
$$\textrm{idf}(w) = \log{\frac{n}{\textrm{df}(w)}}$$

idf can also be justified information-theoretically \lec{4}{2}. Instead of df, we could use collection frequency, but it has been shown to perform worse \lec{4}{2}.

The tf-idf weights are given as a product:
$$\textrm{tf-idf}(w; d) = \textrm{log-tf}(w;d) \cdot \textrm{idf}(w;d) = \log(1 + \textrm{tf}(w; d)) \cdot \log{\frac{n}{\textrm{df}(w)}}$$

The final score of document $d$ is the sum of weights in the query $q$: \lec{4}{3}
$$\mathrm{score}(d, q) = \sum_{w \in q} \textrm{tf-idf}(w; d)$$

We can also treat both documents and queries as vectors in a high-dimensional space of tf-idf weights where each dimension is a term (the vectors are sparse and dimensionality is very high.) Then we can rank documents according to their proximity to the query: \lec{4}{3}
$$\mathrm{score}(d, q) = cos(\vec{q}, \vec{d}) = \frac{\vec{q} \cdot \vec{d}}{\Vert\vec{q}\Vert \Vert\vec{d}\Vert}$$

In practice we use non-symmetric scoring functions.




\subsection{Language models for scoring}
Let's assume the user imagines a relevant document and picks random terms out of it to form a query. Then we want to estimate the distribution of query terms given a document, $P(w|d)$. If we have estimated this, the query probability for a document is
$$P(q|d) = \prod_{w \in q} P(w|d)$$
which becomes the scoring function -- we show the document that was most likely to generate this query. \lec{5}{1}

Pros: simple, easy to estimate $P(w|d)$ based on document collection we have. Cons: too strong assumptions, relevance too strictly tied to probability. \lec{5}{1}

\paragraph{How to estimate $P(q|d)$}
Trivial approach: using term frequency counts \lec{5}{1}. This overfits, though (especially for words that occur very rarely) so we can use smooth using collection frequencies (Jelinek-Mercer smoothing, \lec{5}{1}):

$$P(w|d) = (1-\lambda_d) \hat{P}(w|d) + \lambda_d \hat{P}(w)$$

where $\lambda_d$ can depend on e.g. the length of document $d$. If we differentiate between terms that occur in the document and those that don't, we have a more general approach. \lec{5}{2}

Smoothing is not magic though, it just helps with rare terms. Instead, we can use topic models. \lec{5}{2}

\subsection{Topic models for scoring}
Each document is assumed to be generated as a mixture of topics (using a latent variable for topic assignment). This allows us to express $P(w|d)$ by summing out the latent variable: \lec{5}{2}

$$P(w|d) = \sum_t P(w|t) P(t|d)$$

Two extremes of topic modelling are: a) assuming only one topic (equivalent to a collection model) and b) assuming same amount of topics as documents (equivalent to MLE model).

If we don't know $P(w|t)$, we can estimate it using our collection: iteratively updating our estimate using a multiplicative update rule \lec{5}{4}.

To estimate $P(t|d)$, we can also do multiplicative updates. This converges quickly and can be proven from the Expectation Maximisation algorithm. \lec{5}{3}.

We can get a generative model (i.e. find a probability distribution over documents) of documents using Latent Dirichlet allocation: \lec{5}{5}

$$P(d|params) = \int_{\pi} \textrm{\{prior on such a } \pi\} \cdot \textrm{\{likelihood of d given } \pi \}$$



\begin{enumerate}
	\item
\end{enumerate}
\begin{itemize}
		\item
\end{itemize}

\end{document}
